{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Team : RAGrats\n",
    "# Team Members : Ali Asgar Padaria, Param Patel, Meet Zalavadiya\n",
    "# \n",
    "# Code Description : This file holds code for generating the vectorStore and also prestoring the retrieved contexts via the retriever for easy access for the models.\n",
    "#                    It uses SeneteceTransformer model for embedding the text and FAISS for vector storage.\n",
    "#                    \n",
    "# NLP Concepts Usage: Tokenization, Embeddings\n",
    "#\n",
    "# System : GCP Server L4 GPU\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85cc0fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb45fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\") # NLP Concept: Embeddings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "tokenizer = model.tokenizer\n",
    "core_model = model._first_module().auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c42f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_from_disk(\"../files/train_dataset\")\n",
    "val_dataset = load_from_disk(\"../files/val_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "778c195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling Function\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, dim=1) / torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "\n",
    "# Encode Function\n",
    "def encode_texts(texts, batch_size=8):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in (range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type=device.type, dtype=torch.float16):\n",
    "                model_output = core_model(**encoded_input)\n",
    "                embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        embeddings = embeddings / embeddings.norm(p=2, dim=1, keepdim=True)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Passages from Train and Val Datasets\n",
    "train_contexts = list({context for item in train_dataset for context in item[\"context\"][\"contexts\"]})\n",
    "val_contexts = list({context for item in val_dataset for context in item[\"context\"][\"contexts\"]})\n",
    "\n",
    "# merge train and val contexts\n",
    "all_contexts = list(set(train_contexts) | set(val_contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b94168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3950/3950 [01:23<00:00, 47.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# eoncode all contexts\n",
    "context_embeddings = encode_texts(all_contexts, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fc11066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS Index\n",
    "index = faiss.IndexFlatL2(context_embeddings.shape[1])\n",
    "index.add(context_embeddings)\n",
    "faiss.write_index(index, \"../files/faiss.index\")\n",
    "\n",
    "# Save Contexts\n",
    "with open(\"../files/contexts.json\", \"w\") as f:\n",
    "    json.dump(all_contexts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee00d2",
   "metadata": {},
   "source": [
    "### Evaluate Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d70ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Datasets\n",
    "full_dataset = concatenate_datasets([train_dataset, val_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a62f6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_evaluation(dataset, index, contexts, k=5):\n",
    "    total_matched = 0\n",
    "    total_expected = 0\n",
    "\n",
    "    for item in tqdm(dataset.select(range(20000))):\n",
    "        query = item[\"question\"]\n",
    "        query_embedding = encode_texts([query])[0]\n",
    "        gold_contexts = set(item[\"context\"][\"contexts\"])\n",
    "\n",
    "        _, I = index.search(query_embedding.reshape(1, -1), len(gold_contexts))\n",
    "\n",
    "        retrieved_contexts = {contexts[i] for i in I[0]}\n",
    "        \n",
    "        # Check if any of the retrieved contexts match the answers\n",
    "        matched = len(gold_contexts & retrieved_contexts)\n",
    "        total_matched += matched\n",
    "        total_expected += len(gold_contexts)\n",
    "\n",
    "    return total_matched / total_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59857030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [19:29<00:00, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Accuracy: 0.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieval_accuracy = retrieval_evaluation(full_dataset, index, all_contexts)\n",
    "print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d22fc",
   "metadata": {},
   "source": [
    "The retriever has accuracy of 59% shows on an average around 3 of 5 contexts retreived for each question were in the original context set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1ffaf",
   "metadata": {},
   "source": [
    "### Store Retreived Context Pairs for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1989c6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the FAISS index to retrieve the contexts for the validation set and store them in a dict object\n",
    "\n",
    "def retrieve_for_validation_set(dataset, index, contexts, k=5):\n",
    "    retreived_pairas = []\n",
    "    for item in tqdm(dataset):\n",
    "        query = item[\"question\"]\n",
    "        query_embedding = encode_texts([query])[0]\n",
    "\n",
    "        _, I = index.search(query_embedding.reshape(1, -1), 5)\n",
    "\n",
    "        retrieved_contexts = [contexts[i] for i in I[0]]\n",
    "        \n",
    "        # store question and retireved contexts in a dict object\n",
    "        retreived_pairas.append({\n",
    "            \"question\": query,\n",
    "            \"retrieved_contexts\": retrieved_contexts\n",
    "        })\n",
    "    return retreived_pairas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1a7e960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:55<00:00, 17.25it/s]\n"
     ]
    }
   ],
   "source": [
    "retreived_pairs = retrieve_for_validation_set(val_dataset, index, all_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a625148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this file\n",
    "with open(\"../files/val_retrieved_pairs_base_1.json\", \"w\") as f:\n",
    "    json.dump(retreived_pairs, f)\n",
    "\n",
    "# saved this data for future direct extraction on validation set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
