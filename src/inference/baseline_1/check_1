import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from datasets import load_dataset
from huggingface_hub import login
from tqdm import tqdm
from dotenv import load_dotenv
import os
load_dotenv()
token = os.getenv("HF_TOKEN")



assert torch.cuda.is_available(), "GPU not available!"



def evaluate_(model, data_loader,  yes_token_id, no_token_id, device='cuda'):
    """
    Function to implement zero-shot distilGPT2 inference
    Args:
    model: GPT2 model
    data_loader: Dataloader for the dataset
    ...: Any other arguments you may need

    Returns:
    preds
    """
    model.eval()
    preds = []
    y_true = []
    model.to(device)
    for batch in tqdm(data_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        with torch.no_grad():
            with torch.amp.autocast(device_type=device, dtype=torch.float16):
                output = model(input_ids=input_ids, attention_mask=attention_mask)
                logits = output.logits # (batchsize, chunklen, vocab_size)

                last_indices = attention_mask.sum(-1)-1
                final_logits = logits[torch.arange(len(input_ids)), last_indices] # (batchsize,vocab_size)
                probs = F.softmax(final_logits, dim=-1) # (batchsize,vocab_size)

                yes_probs = probs[:, yes_token_id]
                no_probs = probs[:, no_token_id]

                preds.extend((yes_probs > no_probs).int().tolist())
                y_true.extend(batch['answer'].int().tolist())

    return preds, y_true



login(token=token)
# Load model/tokenizer
model_id = 'meta-llama/Llama-3.2-1B'

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda",
    torch_dtype=torch.float16
)
tokenizer.pad_token = tokenizer.eos_token  # Required
print("Tokenizer and model loaded successfully.")

# Load PubMedQA example
dataset = load_dataset("qiaojin/PubMedQA", "pqa_artificaial")["train"]
# example = dataset['train'][0]

# Prompt with instruction tuning style
def build_prompt(question, context):
    prompt = (
        f"Contexts:\n{context}\n\n"
        "Based on the contexts above, answer the question below with 'Yes', 'No', or 'Maybe'.\n"
        "Then, provide a short explanation that justifies your answer using evidence from the context.\n"
        f"Question: {question}\n"
        f"Answer: "
        )
    
    return prompt

# build a tokenizer

# build a colator
