import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from datasets import load_dataset
from huggingface_hub import login
from tqdm import tqdm
from dotenv import load_dotenv
import os
load_dotenv()
token = os.getenv("HF_TOKEN")



assert torch.cuda.is_available(), "GPU not available!"

login(token=token)
# Load model/tokenizer
model_id = 'meta-llama/Llama-3.2-1B'

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda",
    torch_dtype=torch.float16
)
tokenizer.pad_token = tokenizer.eos_token  # Required
print("Tokenizer and model loaded successfully.")

# Load PubMedQA example
dataset = load_dataset("qiaojin/PubMedQA", "pqa_labeled")["train"]
# example = dataset['train'][0]

# Prompt with instruction tuning style
def build_prompt(question, context):
    prompt = (
        f"Contexts:\n{context}\n\n"
        "Based on the contexts above, answer the question below with 'Yes', 'No', or 'Maybe'.\n"
        "Then, provide a short explanation that justifies your answer using evidence from the context.\n"
        f"Question: {question}\n"
        f"Answer: "
        )
    
    return prompt

# prompt = build_prompt(example['question'], example['context'])

# # Generate
qa_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer)
# response = qa_pipeline(prompt, max_new_tokens=15, do_sample=False, return_full_text=False)[0]['generated_text']

# get device of qa_pipeline
device = qa_pipeline.device
print(f"Pipeline device: {device}")


# # Debug raw output
# print("Raw Generated Text:", repr(response))

# # Postprocess: get Yes/No safely
# tokens = response.strip().split()
# answer = tokens[0] if tokens else "[EMPTY]"
# print("Predicted Answer:", answer)
# print("Ground Truth:", example["final_decision"])

# Evaluation loop
correct = 0
total = 0
results = []

empty_responses = 0

for example in tqdm(dataset.select(range(1000))):
    prompt = build_prompt(example['question'], example['context'])
    try:
        response = qa_pipeline(prompt, max_new_tokens=15, do_sample=False, top_p = 0.5 ,return_full_text=False)[0]['generated_text']
    except Exception as e:
        print("Error:", e)
        response = ""

    pred = response.strip().split()[0].lower() if response.strip() else "[empty]"
    if pred == "[empty]":
        empty_responses += 1

    label = example["final_decision"].lower()


    # Normalize
    if pred.startswith("yes"):
        pred = "yes"
    elif pred.startswith("no"):
        pred = "no"
    elif pred.startswith("maybe"):
        pred = "maybe"

    if pred == label:
        correct += 1

    total += 1
    results.append((pred, label))

accuracy = correct / total
print(f"\nAccuracy on first 1000 examples: {accuracy:.4f}")